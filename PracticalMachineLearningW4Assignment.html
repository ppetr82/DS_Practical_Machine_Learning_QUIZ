<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c14{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Arial";font-style:normal}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Courier New";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Courier New";font-style:normal}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Courier New";font-style:normal}.c11{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c13{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c15{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c8{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;text-align:left}.c5{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;text-align:left}.c19{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c6{background-color:#ffffff;font-size:10.5pt;color:#2775d1;text-decoration:underline}.c16{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c17{font-size:10pt;font-family:"Courier New";font-weight:400}.c18{color:inherit;text-decoration:inherit}.c9{margin-left:144pt}.c7{height:11pt}.c4{margin-left:180pt}.c12{margin-left:108pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c16"><p class="c8 title" id="h.s8xhto1ak7ph"><span class="c14">Practical Machine Learning</span></p><h2 class="c19" id="h.7qj65nf87pl8"><span class="c11">Week 4 Peer-graded Assignment</span></h2><h3 class="c5" id="h.2ttp8oq0u8bw"><span class="c13">Problem description</span></h3><p class="c2"><span>The task of Week 4 Peer-graded Assignment was to use Weight Lifting Exercise Dataset (</span><span class="c6"><a class="c18" href="https://www.google.com/url?q=http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har&amp;sa=D&amp;ust=1510599777663000&amp;usg=AFQjCNEEV_6JMRGqQrUBjxQHpldTn8GJFQ">http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har</a></span><span>), understand the data, define predictive ML model and predict values of &ldquo;classe&rdquo; on the dataset provided by the lecturers (</span><span class="c6"><a class="c18" href="https://www.google.com/url?q=https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv&amp;sa=D&amp;ust=1510599777664000&amp;usg=AFQjCNHre8ECxWK8auv4-rYX8iz1GzMzog">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv</a></span><span class="c1">). </span></p><h3 class="c5" id="h.t90axjd5icu1"><span class="c13">Feature selection</span></h3><p class="c2"><span class="c1">There were in total 160 columns in the dataset. The first 7 columns were dimension data (user_name, timestamps etc.), columns 8 to 159 were factual columns containing the measures and finally the column 160 contained the predicted value (&ldquo;classe&rdquo;). To extract the features I took the columns 8 to 159 and reviewed the values. It seems that if new_window = &ldquo;yes&rdquo; then the values like mean, variance, max, mint etc. were calculated (derived columns) &nbsp;otherwise these were empty (when new_window = &ldquo;no&rdquo;). There were only 406 rows where new_window = &ldquo;true&rdquo; so I decided not to use the derived columns and instead focus on those metrics that contained value for each observation.</span></p><p class="c2"><span class="c1">After filtering out those columns that did not have values for every observation, total 52 columns left as feature candidates. Some of those columns however might be correlated so I calculated the correlation matrix and had a look on all the highly correlated features (cutoff = 0.75). There were in total 21 highly correlated features. It is not necessary to include those into the model - less features will speed up the model training. The final list of the features looks as follows: </span></p><p class="c2"><span class="c0">&nbsp;[1] yaw_belt &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gyros_belt_x &nbsp; &nbsp; &nbsp; &nbsp; gyros_belt_y &nbsp; &nbsp; &nbsp; &nbsp; gyros_belt_z &nbsp; &nbsp; &nbsp; &nbsp;</span></p><p class="c2"><span class="c0">&nbsp;[5] magnet_belt_x &nbsp; &nbsp; &nbsp; &nbsp;magnet_belt_y &nbsp; &nbsp; &nbsp; &nbsp;roll_arm &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; pitch_arm &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c2"><span class="c0">&nbsp;[9] yaw_arm &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;total_accel_arm &nbsp; &nbsp; &nbsp;gyros_arm_y &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;gyros_arm_z &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c2"><span class="c0">[13] magnet_arm_x &nbsp; &nbsp; &nbsp; &nbsp; magnet_arm_z &nbsp; &nbsp; &nbsp; &nbsp; roll_dumbbell &nbsp; &nbsp; &nbsp; &nbsp;pitch_dumbbell &nbsp; &nbsp; &nbsp;</span></p><p class="c2"><span class="c0">[17] yaw_dumbbell &nbsp; &nbsp; &nbsp; &nbsp; total_accel_dumbbell gyros_dumbbell_y &nbsp; &nbsp; magnet_dumbbell_z &nbsp; </span></p><p class="c2"><span class="c0">[21] roll_forearm &nbsp; &nbsp; &nbsp; &nbsp; pitch_forearm &nbsp; &nbsp; &nbsp; &nbsp;yaw_forearm &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;total_accel_forearm </span></p><p class="c2"><span class="c0">[25] gyros_forearm_x &nbsp; &nbsp; &nbsp;gyros_forearm_z &nbsp; &nbsp; &nbsp;accel_forearm_x &nbsp; &nbsp; &nbsp;accel_forearm_z &nbsp; &nbsp; </span></p><p class="c2"><span class="c0">[29] magnet_forearm_x &nbsp; &nbsp; magnet_forearm_y &nbsp; &nbsp; magnet_forearm_z &nbsp; &nbsp;</span></p><p class="c2 c7"><span class="c1"></span></p><p class="c15"><span class="c1">Tab 1.: Final set of features</span></p><p class="c2 c7"><span class="c1"></span></p><p class="c2"><span class="c1">The correlation matrix is on Fig 1. From the plot it is obvious that there no other highly correlated features and we can move to classifier selection. </span></p><h3 class="c5" id="h.n9uwabbfw3tc"><span class="c13">Model selection, training and testing</span></h3><p class="c2"><span class="c1">After selecting the features, we can move to training the first model. I have decided to use decision tree from the RPART package. The model was trained and tested. The overall performance was poor as AUC was around 0.64. In order to improve the results I decided to try out random forest algorithm with ntree parameter set to 10. After training the random forest the results improved significantly. The overall AUC improved to 0.98 which I consider very good. Below there is one confusion matrix for the decision tree and one for the random forest. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 502.50px; height: 463.39px;"><img alt="Rplot06.png" src="images/image1.png" style="width: 750.59px; height: 558.06px; margin-left: -178.30px; margin-top: -94.67px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c15"><span class="c1">Fig 1. Correlation matrix for the final </span></p><p class="c2 c7"><span class="c1"></span></p><p class="c2 c12"><span class="c17">&nbsp;</span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Reference</span></p><p class="c2 c9"><span class="c0">Prediction &nbsp; &nbsp;A &nbsp; &nbsp;B &nbsp; &nbsp;C &nbsp; &nbsp;D &nbsp; &nbsp;E</span></p><p class="c2 c9"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;A 1057 &nbsp; 80 &nbsp;152 &nbsp; 75 &nbsp; 31</span></p><p class="c2 c9"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;B &nbsp;140 &nbsp;529 &nbsp;191 &nbsp; 64 &nbsp; 25</span></p><p class="c2 c9"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;C &nbsp; 25 &nbsp;108 &nbsp;591 &nbsp; 90 &nbsp; 41</span></p><p class="c2 c9"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;D &nbsp; 48 &nbsp; 49 &nbsp;184 &nbsp;456 &nbsp; 67</span></p><p class="c2 c9"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;E &nbsp; 27 &nbsp;103 &nbsp;151 &nbsp;100 &nbsp;520</span></p><p class="c2 c7 c12"><span class="c10"></span></p><p class="c15"><span class="c1">Tab 2.: Confusion matrix for decision tree</span></p><p class="c2 c7"><span class="c10"></span></p><p class="c2 c9"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Reference</span></p><p class="c2 c9"><span class="c0">Prediction &nbsp; &nbsp;A &nbsp; &nbsp;B &nbsp; &nbsp;C &nbsp; &nbsp;D &nbsp; &nbsp;E</span></p><p class="c2 c9"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;A 1392 &nbsp; &nbsp;1 &nbsp; &nbsp;1 &nbsp; &nbsp;1 &nbsp; &nbsp;0</span></p><p class="c2 c9"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;B &nbsp; &nbsp;9 &nbsp;930 &nbsp; &nbsp;7 &nbsp; &nbsp;2 &nbsp; &nbsp;1</span></p><p class="c2 c9"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;C &nbsp; &nbsp;0 &nbsp; 16 &nbsp;833 &nbsp; &nbsp;6 &nbsp; &nbsp;0</span></p><p class="c2 c9"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;D &nbsp; &nbsp;3 &nbsp; &nbsp;1 &nbsp; 16 &nbsp;783 &nbsp; &nbsp;1</span></p><p class="c2 c9"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;E &nbsp; &nbsp;0 &nbsp; &nbsp;0 &nbsp; &nbsp;5 &nbsp; &nbsp;2 &nbsp;894</span></p><p class="c2 c7"><span class="c10"></span></p><p class="c15"><span class="c1">Tab 3.: Confusion matrix for random forest</span></p><h3 class="c5" id="h.p9yo4r86ab6n"><span class="c13">Conclusion</span></h3><p class="c2"><span class="c1">Firstly, I have selected 21 features from the overall dataset. Then I used those features to predict &ldquo;classe&rdquo; using decision tree and random forest. Decision tree did not perform very well so I decided to use random forest with overall AUC of 0.98. The results as predicted by the random forest for the pml-testing.csv are in Tab. 4. &nbsp;</span></p><p class="c2 c7"><span class="c1"></span></p><p class="c2 c4"><span class="c3">&nbsp; &nbsp;prediction</span></p><p class="c2 c4"><span class="c3">1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; B</span></p><p class="c2 c4"><span class="c3">2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; A</span></p><p class="c2 c4"><span class="c3">3 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; B</span></p><p class="c2 c4"><span class="c3">4 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; A</span></p><p class="c2 c4"><span class="c3">5 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; A</span></p><p class="c2 c4"><span class="c3">6 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; E</span></p><p class="c2 c4"><span class="c3">7 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; D</span></p><p class="c2 c4"><span class="c3">8 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; B</span></p><p class="c2 c4"><span class="c3">9 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; A</span></p><p class="c2 c4"><span class="c3">10 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;A</span></p><p class="c2 c4"><span class="c3">11 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;B</span></p><p class="c2 c4"><span class="c3">12 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;C</span></p><p class="c2 c4"><span class="c3">13 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;B</span></p><p class="c2 c4"><span class="c3">14 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;A</span></p><p class="c2 c4"><span class="c3">15 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;E</span></p><p class="c2 c4"><span class="c3">16 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;E</span></p><p class="c2 c4"><span class="c3">17 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;A</span></p><p class="c2 c4"><span class="c3">18 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;B</span></p><p class="c2 c4"><span class="c3">19 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;B</span></p><p class="c2 c4"><span class="c3">20 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;B</span></p><p class="c2 c7"><span class="c1"></span></p><p class="c15"><span class="c1">Tab 4.: Confusion matrix for random forest</span></p><p class="c15 c7"><span class="c1"></span></p><p class="c2 c7"><span class="c1"></span></p><p class="c2 c7"><span class="c10"></span></p><p class="c2 c7"><span class="c10"></span></p><p class="c2 c7"><span class="c1"></span></p></body></html>